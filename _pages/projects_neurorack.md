---
title: "Neurorack"
layout: single
permalink: /projects/neurorack
author_profile: false
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: /images/project2.jpeg
excerpt: "AI based synthesizer"
toc: true
toc_label: "Table of content"
---

<!-- Load Bulma from CDN -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">

<section class="section">
  <div class="container content">

# Neurorack — Deep-Learning based Eurorack Synthesizer

The Neurorack is a fully standalone real-time synthesizer powered by deep learning, built to integrate seamlessly into a modular Eurorack setup. It explores the use of perceptual audio descriptors as controls for generating and sculpting impact sounds—kicks, hits, cinematic booms—in real time, directly from neural synthesis.

📄 [Scientific paper](https://infoscience.epfl.ch/record/291222)  
🎬 [Demo video](https://www.youtube.com/watch?v=64VpQenCHVs)  
💾 [GitHub repository](https://github.com/acids-ircam/neurorack)  
🏆 [NVIDIA Project of the Month](https://developer.nvidia.com/blog/jetson-neurorack-deep-ai-synthesizer/)

---

## Goals

> Our aim is to build the next generation of musical instruments — systems that are expressive, real-time, and creative by nature. AI is not just a tool here, it’s part of the instrument.

We designed the Neurorack with four key principles:

- **Musical**: Generates rich, evolving sounds that would be impossible to synthesize traditionally.
- **Controllable**: Responds directly to CV and gate signals from other modular gear.
- **Real-time**: Synthesizes audio in one pass with near-zero latency.
- **Standalone**: No computer required — it’s a complete instrument.

---

## Model & Generation Strategy

We focused on the generation of **impact sounds**, because they're notoriously hard to synthesize (transients, texture, pitch/noise hybrid nature).

Each sound is generated from a vector of 7 descriptors:

- Loudness  
- Percussivity  
- Noisiness  
- Tone-like quality  
- Richness  
- Brightness  
- Pitch

These can be adjusted via CV inputs or interpolated between two target sounds using real-time control.

<div class="has-text-centered">
  <img src="https://raw.githubusercontent.com/ninon-io/ninon-io.github.io/master/images/steps.png" style="max-width: 100%;">
  <p class="is-size-7">Design process: model selection, compression, and embedding.</p>
</div>

---

## Interface

The Neurorack behaves like any standard Eurorack module:

- **4 CV inputs** to control descriptors
- **2 Gate inputs** to trigger sound and interpolation
- **Mono audio output**
- **1.3" OLED display** for feedback
- **Rotary encoder** for navigation

<div class="has-text-centered">
  <img src="https://raw.githubusercontent.com/ninon-io/ninon-io.github.io/master/images/interface_neurorack.png" style="max-width: 100%;">
  <p class="is-size-7">Front panel of the Neurorack — 11hp Eurorack module.</p>
</div>

---

## Under the Hood

- Jetson Nano for real-time inference (128-core GPU)
- Python multiprocessing architecture
- Sinc-NSF neural model (source-filter inspired)
- Fully trained on a curated impact sound dataset
- Real-time descriptor generation & synthesis

You can read about the compression strategy and lightweight modeling [here](https://ninon-io.github.io/research/phd/).

---

## Audio Example

Here’s a track made entirely with impact sounds generated by the Neurorack:

<audio controls style="width: 100%;">
  <source src="/audio/raster_demo.wav" type="audio/wav">
  Your browser does not support the audio element.
</audio>

---

## Work in Progress

We’re actively developing new features and iterations:

- A proper menu and navigation UI on the OLED
- Replacing the 7 descriptors with a VAE for intuitive latent space exploration
- Removing the Jetson and running the model on CPU only
- Exploring new models for texture, atmospheres, plucks, etc.

---

## Planned Hardware Improvements

We’re working on a second version:

- Raspberry Pi 5 + Hailo-8 AI Accelerator  
- Internal power (no extra supply required)  
- Cleaner internal layout  
- Support for other synthesis models

---

## Recognition

The project has been featured on:

- [Synthtopia](https://www.synthtopia.com/content/2022/01/08/new-neurorack-module-brings-artificial-intelligence-to-your-eurorack-system/)
- [Sonic State](https://sonicstate.com/news/2022/01/10/the-first-deep-ai-based-synthesizer/)
- [Matrixsynth](https://www.matrixsynth.com/2022/01/acids-neurorack-first-deep-ai-based.html)
- [Keyboards.de](https://www.keyboards.de/equipment/neurorack-synthesizer-mit-kuenstlicher-intelligenz/)

---

## 🎤 Exhibitions

We’ve presented the Neurorack at:

- **Superbooth (Berlin)**
- **SynthFest France (Nantes)**
- **Manifest @ IRCAM**, with Raster Noton artists

Despite some prototype limitations (startup delay, external power, descriptor hard-coding), we’ve received strong and enthusiastic feedback from musicians, developers, and modular heads alike.

---

## Figures & Visuals

Below are placeholders for model architecture and descriptor comparisons. You can replace them with actual content later.

<div class="columns is-multiline">
  <div class="column is-half">
    <img src="/images/PLACEHOLDER_MODEL_ARCH.png">
    <p class="is-size-7">Model architecture (placeholder)</p>
  </div>
  <div class="column is-half">
    <img src="/images/PLACEHOLDER_SPECTROGRAM.png">
    <p class="is-size-7">Spectrogram comparison (placeholder)</p>
  </div>
</div>

---

## Next Steps

- Embedding FRAVE (full descriptor-based timbre model)
- Expanding descriptor mappings to allow external training or user design
- Showcase at **Sónar+D 2025**
- Open-sourcing model presets and mappings

---

## Read More

- [Full research chapter](https://ninon-io.github.io/research/phd/)
- [Github](https://github.com/acids-ircam/neurorack)
- [Demo audio](https://ninon-io.github.io/projects/neurorack)

</div>
</section>




